{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        " # install PySpark\n",
        "!pip install pyspark\n",
        "!pip install spark-nlp\n",
        "!pip install nltk\n",
        "!pip install plotly\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T06:28:57.1947209Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T06:28:57.288406Z",
              "execution_finish_time": "2023-11-29T06:29:20.3494916Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "637b6d46-2926-4c5e-820a-9934489ac422"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 8, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: pyspark in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (3.2.1)\nRequirement already satisfied: py4j==0.10.9.3 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from pyspark) (0.10.9.3)\nCollecting spark-nlp\n  Downloading spark_nlp-5.1.4-py2.py3-none-any.whl (540 kB)\n\u001b[K     |████████████████████████████████| 540 kB 9.3 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: spark-nlp\nSuccessfully installed spark-nlp-5.1.4\nRequirement already satisfied: nltk in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (3.6.2)\nRequirement already satisfied: joblib in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from nltk) (1.0.1)\nRequirement already satisfied: regex in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from nltk) (2021.7.6)\nRequirement already satisfied: tqdm in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from nltk) (4.61.2)\nRequirement already satisfied: click in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from nltk) (8.0.1)\nRequirement already satisfied: plotly in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (4.14.3)\nRequirement already satisfied: retrying>=1.3.3 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from plotly) (1.3.3)\nRequirement already satisfied: six in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from plotly) (1.16.0)\nRequirement already satisfied: matplotlib in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (3.4.2)\nRequirement already satisfied: pillow>=6.2.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from matplotlib) (8.2.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from matplotlib) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from matplotlib) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: numpy>=1.16 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from matplotlib) (1.19.4)\nRequirement already satisfied: python-dateutil>=2.7 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from matplotlib) (2.8.1)\nRequirement already satisfied: six in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\nRequirement already satisfied: scikit-learn in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (0.23.2)\nRequirement already satisfied: numpy>=1.13.3 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from scikit-learn) (1.19.4)\nRequirement already satisfied: joblib>=0.11 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from scikit-learn) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\nRequirement already satisfied: scipy>=0.19.1 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from scikit-learn) (1.5.3)\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PhiPwQ3cOm-",
        "outputId": "550e78b9-de2f-45ad-8786-e22903e505bf",
        "gather": {
          "logged": 1701239360509
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%configure -f \\\n",
        "{\"conf\": {\"spark.jars.packages\": \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.2\"}}"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T06:26:01.19209Z",
              "session_start_time": "2023-11-29T06:26:01.3088279Z",
              "execution_start_time": "2023-11-29T06:27:49.3425874Z",
              "execution_finish_time": "2023-11-29T06:27:49.3719022Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "461ca389-14e4-497c-bb32-374c0f9ca7ae"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Unrecognized options: "
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T06:27:51.807459Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T06:28:21.6157505Z",
              "execution_finish_time": "2023-11-29T06:28:21.9289208Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "15822d54-e4a9-4f40-b080-08f5448234c3"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "<pyspark.sql.session.SparkSession at 0x7f36edc7c7f0>",
            "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://vm-da734808:44151\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.2.5.1-100879434</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Azure ML Experiment</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701239302099
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T06:29:25.0536092Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T06:29:25.1576947Z",
              "execution_finish_time": "2023-11-29T06:29:25.4595829Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "76b75dec-d5dc-4b15-a155-8ca6ba6c0f7b"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "id": "9x0awa2mu-VR",
        "gather": {
          "logged": 1701239365572
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T06:29:27.5743407Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T06:29:27.6626688Z",
              "execution_finish_time": "2023-11-29T06:29:27.9703537Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "15419bb7-d672-4131-970b-da75dc2f724a"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 10, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "'/synfs/notebook/49/aml_notebook_mount'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701239368129
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir_name = \"Users/fall-2023-reddit-project-team-02/data/csv/\"\n",
        "df = pd.read_csv(dir_name + \"nlp_reddit_df.csv\")\n",
        "print(df.shape)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T06:31:47.6322885Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T06:31:47.7476349Z",
              "execution_finish_time": "2023-11-29T06:32:34.5849317Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "49b5b1d7-5eae-4c31-b9c7-ad3a6a19dc5a"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 13, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(217394, 57)\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "-zy2iLQyi7pW",
        "outputId": "f8366453-f397-4694-ddb0-d9e52c425994",
        "gather": {
          "logged": 1701239554738
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = df.copy(deep=True)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 16,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T06:33:21.7905292Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T06:33:21.8859037Z",
              "execution_finish_time": "2023-11-29T06:33:22.1878655Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "fcfd29de-c0dd-430b-91e3-7b0eac24b429"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 16, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701239602296
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean Data for ML Pipeline"
      ],
      "metadata": {
        "id": "-CQo9Ma_leBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.selftext.isna().sum()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T06:32:42.8383305Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T06:32:42.93215Z",
              "execution_finish_time": "2023-11-29T06:32:43.2365043Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "3f5ef3bd-0789-40e3-a0f5-72014d95e3ca"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 15, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 29,
          "data": {
            "text/plain": "82012"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701239563377
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove rows that don't have text in selftext\n",
        "df = df.dropna(subset=['selftext'])\n",
        "print(df.shape)\n",
        "\n",
        "# remove sentence embeddings which was already in the nlp df\n",
        "df = df.drop('sentence_embeddings', axis = 'columns')\n",
        "print(df.shape)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T06:33:24.9678507Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T06:33:25.08063Z",
              "execution_finish_time": "2023-11-29T06:33:25.9182233Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "24cae1cc-98ff-449b-a9cc-8e9874bb68ec"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 17, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(135382, 57)\n(135382, 56)\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701239606042
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract string from sentiment analysis\n",
        "df.loc[:, 'sentiment_label'] = df.loc[:, 'sentiment'].apply(lambda x: x[2:len(x)-2])\n",
        "# convert boolean fields into integer fields\n",
        "df.loc[:, ['airbnb', 'rent', 'gentrification', 'public_transit', 'tourists']] = df.loc[:, ['airbnb', 'rent', 'gentrification', 'public_transit', 'tourists']].astype(int)\n",
        "\n",
        "# subset to only positive and negative since doing binary classification\n",
        "sentiment_df = df[df.sentiment_label.apply(lambda x: x in ['pos', 'neg'])]\n",
        "sentiment_df.loc[:,'sentiment_num'] = sentiment_df['sentiment_label'].apply(lambda x: 1 if 'pos' else 0)\n",
        "sentiment_df.sentiment_label.value_counts()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 24,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T06:36:01.7975586Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T06:36:01.8894835Z",
              "execution_finish_time": "2023-11-29T06:36:05.4079152Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "b54d8042-0c19-4eb1-8387-d81fcc9f7163"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 24, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages/pandas/core/indexing.py:1597: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self.obj[key] = value\n/home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages/pandas/core/indexing.py:1676: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(ilocs[0], value, pi)\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 47,
          "data": {
            "text/plain": "pos    76152\nneg    34071\nName: sentiment_label, dtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701239765568
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a sample of the dataframe due to size limit constraints\n",
        "frac_samp = .50\n",
        "\n",
        "# for sentiment\n",
        "df_sent_sample = sentiment_df.sample(frac = frac_samp, random_state = 22)\n",
        "print(df_sent_sample.shape)\n",
        "spark_sent = spark.createDataFrame(df_sent_sample) \n",
        "\n",
        "# for subreddit\n",
        "df_sub_sample = df.sample(frac = frac_samp, random_state = 22)\n",
        "spark_sub = spark.createDataFrame(df_sub_sample) \n",
        "print(df_sub_sample.shape)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 27,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T06:38:31.5297493Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T06:38:31.6524536Z",
              "execution_finish_time": "2023-11-29T06:38:33.1298387Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "5f16f3ea-fa77-4e1f-b070-0c5381ee9512"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 27, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(11022, 58)\n(13538, 57)\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701239913296
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# from pyspark.ml import Pipeline\n",
        "# from sparknlp.base import DocumentAssembler, SentenceDetector, Tokenizer, Normalizer, StopWordsCleaner\n",
        "# from sparknlp.annotator import UniversalSentenceEncoder\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "# from pyspark.ml.classification import SVMModel, SVM\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier, MultilayerPerceptronClassifier, GBTClassifier\n",
        "from pyspark.ml import Pipeline, Model"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 38,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T06:43:04.5567312Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T06:43:04.6880742Z",
              "execution_finish_time": "2023-11-29T06:43:04.9716223Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "d361dc8c-21f2-4fe9-8a8f-759bccfe0b4c"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 38, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701240185091
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree w/ 8 Features; Predicting Subreddit\n",
        "\n",
        "We started off initially only using 2 features in a Decision Tree in order to analyze computation time and to establish a **very** simple baseline model. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the categorical labels in the 'Species' column to numerical values\n",
        "label_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\n",
        "data = label_indexer.fit(spark_sub).transform(spark_sub)\n",
        "\n",
        "# Assemble the feature columns into a single vector column\n",
        "# assembler = VectorAssembler(inputCols=[\"num_words\", \"score\"], outputCol=\"features\")\n",
        "assembler = VectorAssembler(inputCols=['num_comments', \"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\n",
        "data = assembler.transform(data)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
        "dt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "\n",
        "model = dt_classifier.fit(train_data)\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator_acc.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "evaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "roc_auc = evaluator_prec.evaluate(predictions)\n",
        "print(f\"Test Weighted Precision: {roc_auc:.2f}\")\n",
        "\n",
        "evaluator_rec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "roc_auc = evaluator_rec.evaluate(predictions)\n",
        "print(f\"Test Weighted Recall: {roc_auc:.2f}\")\n",
        "\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions, {evaluator_f1.metricName: \"f1\"})\n",
        "print(f\"Test F1: {f1_score:.2f}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 103,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T07:53:34.6218571Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T07:53:34.7496877Z",
              "execution_finish_time": "2023-11-29T07:53:43.4240769Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 14,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 582,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 103:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\n# assembler = VectorAssembler(inputCols=[\"num_words\", \"score\"], outputCol=\"features\")\nassembler = VectorAssembler(inputCols=['num_comments', \"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\ndt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\nmodel = dt_classifier.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\"...",
                    "submissionTime": "2023-11-29T07:53:41.890GMT",
                    "completionTime": "2023-11-29T07:53:42.448GMT",
                    "stageIds": [
                      1191,
                      1190
                    ],
                    "jobGroup": "103",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 581,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 103:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\n# assembler = VectorAssembler(inputCols=[\"num_words\", \"score\"], outputCol=\"features\")\nassembler = VectorAssembler(inputCols=['num_comments', \"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\ndt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\nmodel = dt_classifier.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\"...",
                    "submissionTime": "2023-11-29T07:53:41.237GMT",
                    "completionTime": "2023-11-29T07:53:41.803GMT",
                    "stageIds": [
                      1188,
                      1189
                    ],
                    "jobGroup": "103",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 580,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 103:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\n# assembler = VectorAssembler(inputCols=[\"num_words\", \"score\"], outputCol=\"features\")\nassembler = VectorAssembler(inputCols=['num_comments', \"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\ndt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\nmodel = dt_classifier.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\"...",
                    "submissionTime": "2023-11-29T07:53:40.606GMT",
                    "completionTime": "2023-11-29T07:53:41.167GMT",
                    "stageIds": [
                      1186,
                      1187
                    ],
                    "jobGroup": "103",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 579,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 103:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\n# assembler = VectorAssembler(inputCols=[\"num_words\", \"score\"], outputCol=\"features\")\nassembler = VectorAssembler(inputCols=['num_comments', \"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\ndt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\nmodel = dt_classifier.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\"...",
                    "submissionTime": "2023-11-29T07:53:39.904GMT",
                    "completionTime": "2023-11-29T07:53:40.535GMT",
                    "stageIds": [
                      1184,
                      1185
                    ],
                    "jobGroup": "103",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:663",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 578,
                    "name": "collectAsMap at RandomForest.scala:663",
                    "description": "Job group for statement 103:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\n# assembler = VectorAssembler(inputCols=[\"num_words\", \"score\"], outputCol=\"features\")\nassembler = VectorAssembler(inputCols=['num_comments', \"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\ndt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\nmodel = dt_classifier.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\"...",
                    "submissionTime": "2023-11-29T07:53:39.132GMT",
                    "completionTime": "2023-11-29T07:53:39.672GMT",
                    "stageIds": [
                      1182,
                      1183
                    ],
                    "jobGroup": "103",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:663",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 577,
                    "name": "collectAsMap at RandomForest.scala:663",
                    "description": "Job group for statement 103:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\n# assembler = VectorAssembler(inputCols=[\"num_words\", \"score\"], outputCol=\"features\")\nassembler = VectorAssembler(inputCols=['num_comments', \"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\ndt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\nmodel = dt_classifier.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\"...",
                    "submissionTime": "2023-11-29T07:53:38.597GMT",
                    "completionTime": "2023-11-29T07:53:39.105GMT",
                    "stageIds": [
                      1181,
                      1180
                    ],
                    "jobGroup": "103",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:663",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 576,
                    "name": "collectAsMap at RandomForest.scala:663",
                    "description": "Job group for statement 103:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\n# assembler = VectorAssembler(inputCols=[\"num_words\", \"score\"], outputCol=\"features\")\nassembler = VectorAssembler(inputCols=['num_comments', \"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\ndt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\nmodel = dt_classifier.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\"...",
                    "submissionTime": "2023-11-29T07:53:38.062GMT",
                    "completionTime": "2023-11-29T07:53:38.574GMT",
                    "stageIds": [
                      1178,
                      1179
                    ],
                    "jobGroup": "103",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:663",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 575,
                    "name": "collectAsMap at RandomForest.scala:663",
                    "description": "Job group for statement 103:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\n# assembler = VectorAssembler(inputCols=[\"num_words\", \"score\"], outputCol=\"features\")\nassembler = VectorAssembler(inputCols=['num_comments', \"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\ndt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\nmodel = dt_classifier.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\"...",
                    "submissionTime": "2023-11-29T07:53:37.549GMT",
                    "completionTime": "2023-11-29T07:53:38.043GMT",
                    "stageIds": [
                      1176,
                      1177
                    ],
                    "jobGroup": "103",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:663",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 574,
                    "name": "collectAsMap at RandomForest.scala:663",
                    "description": "Job group for statement 103:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\n# assembler = VectorAssembler(inputCols=[\"num_words\", \"score\"], outputCol=\"features\")\nassembler = VectorAssembler(inputCols=['num_comments', \"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\ndt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\nmodel = dt_classifier.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\"...",
                    "submissionTime": "2023-11-29T07:53:36.949GMT",
                    "completionTime": "2023-11-29T07:53:37.527GMT",
                    "stageIds": [
                      1174,
                      1175
                    ],
                    "jobGroup": "103",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:1054",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 573,
                    "name": "collectAsMap at RandomForest.scala:1054",
                    "description": "Job group for statement 103:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\n# assembler = VectorAssembler(inputCols=[\"num_words\", \"score\"], outputCol=\"features\")\nassembler = VectorAssembler(inputCols=['num_comments', \"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\ndt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\nmodel = dt_classifier.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\"...",
                    "submissionTime": "2023-11-29T07:53:36.325GMT",
                    "completionTime": "2023-11-29T07:53:36.921GMT",
                    "stageIds": [
                      1173,
                      1172
                    ],
                    "jobGroup": "103",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "aggregate at DecisionTreeMetadata.scala:125",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 572,
                    "name": "aggregate at DecisionTreeMetadata.scala:125",
                    "description": "Job group for statement 103:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\n# assembler = VectorAssembler(inputCols=[\"num_words\", \"score\"], outputCol=\"features\")\nassembler = VectorAssembler(inputCols=['num_comments', \"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\ndt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\nmodel = dt_classifier.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\"...",
                    "submissionTime": "2023-11-29T07:53:35.691GMT",
                    "completionTime": "2023-11-29T07:53:36.300GMT",
                    "stageIds": [
                      1171
                    ],
                    "jobGroup": "103",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "take at DecisionTreeMetadata.scala:119",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 571,
                    "name": "take at DecisionTreeMetadata.scala:119",
                    "description": "Job group for statement 103:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\n# assembler = VectorAssembler(inputCols=[\"num_words\", \"score\"], outputCol=\"features\")\nassembler = VectorAssembler(inputCols=['num_comments', \"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\ndt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\nmodel = dt_classifier.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\"...",
                    "submissionTime": "2023-11-29T07:53:35.552GMT",
                    "completionTime": "2023-11-29T07:53:35.684GMT",
                    "stageIds": [
                      1170
                    ],
                    "jobGroup": "103",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at StringIndexer.scala:204",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 570,
                    "name": "collect at StringIndexer.scala:204",
                    "description": "Job group for statement 103:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\n# assembler = VectorAssembler(inputCols=[\"num_words\", \"score\"], outputCol=\"features\")\nassembler = VectorAssembler(inputCols=['num_comments', \"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\ndt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\nmodel = dt_classifier.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\"...",
                    "submissionTime": "2023-11-29T07:53:35.271GMT",
                    "completionTime": "2023-11-29T07:53:35.298GMT",
                    "stageIds": [
                      1168,
                      1169
                    ],
                    "jobGroup": "103",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 8,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at StringIndexer.scala:204",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 569,
                    "name": "collect at StringIndexer.scala:204",
                    "description": "Job group for statement 103:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\n# assembler = VectorAssembler(inputCols=[\"num_words\", \"score\"], outputCol=\"features\")\nassembler = VectorAssembler(inputCols=['num_comments', \"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\ndt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\nmodel = dt_classifier.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\"...",
                    "submissionTime": "2023-11-29T07:53:34.771GMT",
                    "completionTime": "2023-11-29T07:53:35.256GMT",
                    "stageIds": [
                      1167
                    ],
                    "jobGroup": "103",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "d54226da-4542-4284-b811-019e76d43454"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 103, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test Accuracy: 0.43\nTest Weighted Precision: 0.41\nTest Weighted Recall: 0.43\nTest F1: 0.40\n"
        }
      ],
      "execution_count": 101,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701244423578
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"Users/fall-2023-reddit-project-team-02/data/models/rf-sub\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 106,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T08:00:38.5335108Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T08:00:38.6549164Z",
              "execution_finish_time": "2023-11-29T08:00:38.9893056Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "a420e9d8-a631-4681-a0c1-4c05c853ece0"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 106, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o14508.save.\n: java.io.IOException: Path Users/fall-2023-reddit-project-team-02/data/models/rf-sub already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_5137/3091678336.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Users/fall-2023-reddit-project-team-02/data/models/rf-sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a string, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o14508.save.\n: java.io.IOException: Path Users/fall-2023-reddit-project-team-02/data/models/rf-sub already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
          ]
        }
      ],
      "execution_count": 104,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701244839134
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = model.featureImportances.toArray()\n",
        "\n",
        "# Show feature importance\n",
        "for i, column in enumerate(assembler.getInputCols()):\n",
        "    print(f\"Feature '{column}': {feature_importance[i]:.2f}\")\n",
        "\n",
        "dt_sub_pred = predictions.toPandas()\n",
        "dt_sub_pred.to_csv(dir_name + \"pred_sub_dt.csv\", index = False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 90,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T07:40:37.4820693Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T07:40:46.1587704Z",
              "execution_finish_time": "2023-11-29T07:40:49.7318173Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_5137/4206224974.py:7",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 455,
                    "name": "toPandas at /tmp/ipykernel_5137/4206224974.py:7",
                    "description": "Job group for statement 90:\nfeature_importance = model.featureImportances.toArray()\n\n# Show feature importance\nfor i, column in enumerate(assembler.getInputCols()):\n    print(f\"Feature '{column}': {feature_importance[i]:.2f}\")\n\ndt_sub_pred = predictions.toPandas()\ndt_sub_pred.to_csv(dir_name + \"pred_sub_dt.csv\", index = False)",
                    "submissionTime": "2023-11-29T07:40:46.256GMT",
                    "completionTime": "2023-11-29T07:40:46.858GMT",
                    "stageIds": [
                      940
                    ],
                    "jobGroup": "90",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "18191948-eaf7-44b9-9507-cadd23f574be"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 90, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Feature 'num_comments': 0.60\nFeature 'num_words': 0.13\nFeature 'score': 0.26\nFeature 'airbnb': 0.00\nFeature 'rent': 0.00\nFeature 'gentrification': 0.00\nFeature 'public_transit': 0.00\nFeature 'tourists': 0.00\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n  Unsupported type in conversion to Arrow: VectorUDT\nAttempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n  warnings.warn(msg)\n"
        }
      ],
      "execution_count": 88,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701243649789
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest w/ Two Features; Predicting Subreddit"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the categorical labels in the 'Species' column to numerical values\n",
        "label_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\n",
        "data = label_indexer.fit(spark_sub).transform(spark_sub)\n",
        "\n",
        "# Assemble the feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\n",
        "data = assembler.transform(data)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n",
        "\n",
        "model = rf.fit(train_data)\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator_acc.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "evaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "roc_auc = evaluator_prec.evaluate(predictions)\n",
        "print(f\"Test Weighted Precision: {roc_auc:.2f}\")\n",
        "\n",
        "evaluator_rec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "roc_auc = evaluator_rec.evaluate(predictions)\n",
        "print(f\"Test Weighted Recall: {roc_auc:.2f}\")\n",
        "\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions, {evaluator_f1.metricName: \"f1\"})\n",
        "print(f\"Test F1: {f1_score:.2f}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 101,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T07:52:00.8500023Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T07:52:00.9491631Z",
              "execution_finish_time": "2023-11-29T07:52:11.5746854Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 14,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 567,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 101:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n\nmodel = rf.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\nevaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=...",
                    "submissionTime": "2023-11-29T07:52:08.806GMT",
                    "completionTime": "2023-11-29T07:52:09.359GMT",
                    "stageIds": [
                      1164,
                      1165
                    ],
                    "jobGroup": "101",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 566,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 101:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n\nmodel = rf.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\nevaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=...",
                    "submissionTime": "2023-11-29T07:52:08.148GMT",
                    "completionTime": "2023-11-29T07:52:08.716GMT",
                    "stageIds": [
                      1163,
                      1162
                    ],
                    "jobGroup": "101",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 565,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 101:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n\nmodel = rf.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\nevaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=...",
                    "submissionTime": "2023-11-29T07:52:07.347GMT",
                    "completionTime": "2023-11-29T07:52:08.069GMT",
                    "stageIds": [
                      1160,
                      1161
                    ],
                    "jobGroup": "101",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 564,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 101:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n\nmodel = rf.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\nevaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=...",
                    "submissionTime": "2023-11-29T07:52:06.536GMT",
                    "completionTime": "2023-11-29T07:52:07.187GMT",
                    "stageIds": [
                      1159,
                      1158
                    ],
                    "jobGroup": "101",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:663",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 563,
                    "name": "collectAsMap at RandomForest.scala:663",
                    "description": "Job group for statement 101:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n\nmodel = rf.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\nevaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=...",
                    "submissionTime": "2023-11-29T07:52:05.733GMT",
                    "completionTime": "2023-11-29T07:52:06.270GMT",
                    "stageIds": [
                      1156,
                      1157
                    ],
                    "jobGroup": "101",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:663",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 562,
                    "name": "collectAsMap at RandomForest.scala:663",
                    "description": "Job group for statement 101:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n\nmodel = rf.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\nevaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=...",
                    "submissionTime": "2023-11-29T07:52:05.136GMT",
                    "completionTime": "2023-11-29T07:52:05.707GMT",
                    "stageIds": [
                      1155,
                      1154
                    ],
                    "jobGroup": "101",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:663",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 561,
                    "name": "collectAsMap at RandomForest.scala:663",
                    "description": "Job group for statement 101:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n\nmodel = rf.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\nevaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=...",
                    "submissionTime": "2023-11-29T07:52:04.572GMT",
                    "completionTime": "2023-11-29T07:52:05.111GMT",
                    "stageIds": [
                      1152,
                      1153
                    ],
                    "jobGroup": "101",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:663",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 560,
                    "name": "collectAsMap at RandomForest.scala:663",
                    "description": "Job group for statement 101:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n\nmodel = rf.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\nevaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=...",
                    "submissionTime": "2023-11-29T07:52:04.036GMT",
                    "completionTime": "2023-11-29T07:52:04.550GMT",
                    "stageIds": [
                      1150,
                      1151
                    ],
                    "jobGroup": "101",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:663",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 559,
                    "name": "collectAsMap at RandomForest.scala:663",
                    "description": "Job group for statement 101:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n\nmodel = rf.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\nevaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=...",
                    "submissionTime": "2023-11-29T07:52:03.385GMT",
                    "completionTime": "2023-11-29T07:52:04.016GMT",
                    "stageIds": [
                      1148,
                      1149
                    ],
                    "jobGroup": "101",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:1054",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 558,
                    "name": "collectAsMap at RandomForest.scala:1054",
                    "description": "Job group for statement 101:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n\nmodel = rf.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\nevaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=...",
                    "submissionTime": "2023-11-29T07:52:02.749GMT",
                    "completionTime": "2023-11-29T07:52:03.362GMT",
                    "stageIds": [
                      1146,
                      1147
                    ],
                    "jobGroup": "101",
                    "status": "SUCCEEDED",
                    "numTasks": 15,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 15,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 15,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "aggregate at DecisionTreeMetadata.scala:125",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 557,
                    "name": "aggregate at DecisionTreeMetadata.scala:125",
                    "description": "Job group for statement 101:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n\nmodel = rf.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\nevaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=...",
                    "submissionTime": "2023-11-29T07:52:02.145GMT",
                    "completionTime": "2023-11-29T07:52:02.727GMT",
                    "stageIds": [
                      1145
                    ],
                    "jobGroup": "101",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "take at DecisionTreeMetadata.scala:119",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 556,
                    "name": "take at DecisionTreeMetadata.scala:119",
                    "description": "Job group for statement 101:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n\nmodel = rf.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\nevaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=...",
                    "submissionTime": "2023-11-29T07:52:01.999GMT",
                    "completionTime": "2023-11-29T07:52:02.140GMT",
                    "stageIds": [
                      1144
                    ],
                    "jobGroup": "101",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at StringIndexer.scala:204",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 555,
                    "name": "collect at StringIndexer.scala:204",
                    "description": "Job group for statement 101:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n\nmodel = rf.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\nevaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=...",
                    "submissionTime": "2023-11-29T07:52:01.513GMT",
                    "completionTime": "2023-11-29T07:52:01.694GMT",
                    "stageIds": [
                      1142,
                      1143
                    ],
                    "jobGroup": "101",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 8,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at StringIndexer.scala:204",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 554,
                    "name": "collect at StringIndexer.scala:204",
                    "description": "Job group for statement 101:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sub).transform(spark_sub)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n\nmodel = rf.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\nevaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=...",
                    "submissionTime": "2023-11-29T07:52:00.994GMT",
                    "completionTime": "2023-11-29T07:52:01.490GMT",
                    "stageIds": [
                      1141
                    ],
                    "jobGroup": "101",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "9ac7ee59-6a57-439f-93ad-9a6c7b4685bb"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 101, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test Accuracy: 0.40\nTest Weighted Precision: 0.39\nTest Weighted Recall: 0.40\nTest F1: 0.36\n"
        }
      ],
      "execution_count": 99,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701244331735
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = model.featureImportances.toArray()\n",
        "\n",
        "# Show feature importance\n",
        "for i, column in enumerate(assembler.getInputCols()):\n",
        "    print(f\"Feature '{column}': {feature_importance[i]:.2f}\")\n",
        "\n",
        "sub_pred = predictions.toPandas()\n",
        "sub_pred.to_csv(dir_name + \"pred_sub_rf.csv\", index = False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 102,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T07:52:15.7089327Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T07:52:15.8020331Z",
              "execution_finish_time": "2023-11-29T07:52:18.3339675Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_5137/892087462.py:7",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 568,
                    "name": "toPandas at /tmp/ipykernel_5137/892087462.py:7",
                    "description": "Job group for statement 102:\nfeature_importance = model.featureImportances.toArray()\n\n# Show feature importance\nfor i, column in enumerate(assembler.getInputCols()):\n    print(f\"Feature '{column}': {feature_importance[i]:.2f}\")\n\nsub_pred = predictions.toPandas()\nsub_pred.to_csv(dir_name + \"pred_sub_rf.csv\", index = False)",
                    "submissionTime": "2023-11-29T07:52:15.902GMT",
                    "completionTime": "2023-11-29T07:52:16.493GMT",
                    "stageIds": [
                      1166
                    ],
                    "jobGroup": "102",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "70ab7b13-6051-4847-a77c-f3ed3fa540b1"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 102, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Feature 'num_words': 0.32\nFeature 'score': 0.63\nFeature 'airbnb': 0.00\nFeature 'rent': 0.03\nFeature 'gentrification': 0.00\nFeature 'public_transit': 0.02\nFeature 'tourists': 0.00\n"
        }
      ],
      "execution_count": 100,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701244338387
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM; predicting sentiment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the categorical labels in the 'sentiment label' column to numerical values\n",
        "label_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\n",
        "data = label_indexer.fit(spark_sent).transform(spark_sent)\n",
        "\n",
        "# Assemble the feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\n",
        "data = assembler.transform(data)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
        "svm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "print(\"Starting to fit\")\n",
        "model = svm_classifier.fit(train_data)\n",
        "print(\"finished fitting model\")\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "\n",
        "evaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "accuracy = evaluator_acc.evaluate(predictions)\n",
        "\n",
        "print(f\"Test areaUnderROC: {accuracy:.2f}\")\n",
        "\n",
        "evaluator_prec = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderPR\")\n",
        "roc_auc = evaluator_prec.evaluate(predictions)\n",
        "print(f\"Test areaUnderPR: {roc_auc:.2f}\")\n",
        "\n",
        "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator_acc.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "evaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "roc_auc = evaluator_prec.evaluate(predictions)\n",
        "print(f\"Test Weighted Precision: {roc_auc:.2f}\")\n",
        "\n",
        "evaluator_rec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "roc_auc = evaluator_rec.evaluate(predictions)\n",
        "print(f\"Test Weighted Recall: {roc_auc:.2f}\")\n",
        "\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions, {evaluator_f1.metricName: \"f1\"})\n",
        "print(f\"Test F1: {f1_score:.2f}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 93,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T07:40:37.6405816Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T07:41:03.8663898Z",
              "execution_finish_time": "2023-11-29T07:41:16.7207385Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 29,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 499,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:13.921GMT",
                    "completionTime": "2023-11-29T07:41:14.321GMT",
                    "stageIds": [
                      1029,
                      1030
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 498,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:13.414GMT",
                    "completionTime": "2023-11-29T07:41:13.842GMT",
                    "stageIds": [
                      1027,
                      1028
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 497,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:12.893GMT",
                    "completionTime": "2023-11-29T07:41:13.344GMT",
                    "stageIds": [
                      1025,
                      1026
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 496,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:12.318GMT",
                    "completionTime": "2023-11-29T07:41:12.747GMT",
                    "stageIds": [
                      1023,
                      1024
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at AreaUnderCurve.scala:44",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 495,
                    "name": "collect at AreaUnderCurve.scala:44",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:12.210GMT",
                    "completionTime": "2023-11-29T07:41:12.226GMT",
                    "stageIds": [
                      1021,
                      1022,
                      1020
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 18,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 16,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at BinaryClassificationMetrics.scala:135",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 494,
                    "name": "first at BinaryClassificationMetrics.scala:135",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:12.185GMT",
                    "completionTime": "2023-11-29T07:41:12.202GMT",
                    "stageIds": [
                      1017,
                      1018,
                      1019
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 17,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 16,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at BinaryClassificationMetrics.scala:237",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 493,
                    "name": "collect at BinaryClassificationMetrics.scala:237",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:12.155GMT",
                    "completionTime": "2023-11-29T07:41:12.175GMT",
                    "stageIds": [
                      1014,
                      1015,
                      1016
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 18,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 16,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at BinaryClassificationMetrics.scala:197",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 492,
                    "name": "count at BinaryClassificationMetrics.scala:197",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:12.098GMT",
                    "completionTime": "2023-11-29T07:41:12.145GMT",
                    "stageIds": [
                      1011,
                      1012,
                      1013
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 18,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 8,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "sortByKey at BinaryClassificationMetrics.scala:189",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 491,
                    "name": "sortByKey at BinaryClassificationMetrics.scala:189",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:11.579GMT",
                    "completionTime": "2023-11-29T07:41:12.088GMT",
                    "stageIds": [
                      1009,
                      1010
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at AreaUnderCurve.scala:44",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 490,
                    "name": "collect at AreaUnderCurve.scala:44",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:11.498GMT",
                    "completionTime": "2023-11-29T07:41:11.513GMT",
                    "stageIds": [
                      1007,
                      1008,
                      1006
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 18,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 16,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at BinaryClassificationMetrics.scala:237",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 489,
                    "name": "collect at BinaryClassificationMetrics.scala:237",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:11.469GMT",
                    "completionTime": "2023-11-29T07:41:11.484GMT",
                    "stageIds": [
                      1003,
                      1004,
                      1005
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 18,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 16,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at BinaryClassificationMetrics.scala:197",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 488,
                    "name": "count at BinaryClassificationMetrics.scala:197",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:11.410GMT",
                    "completionTime": "2023-11-29T07:41:11.458GMT",
                    "stageIds": [
                      1002,
                      1000,
                      1001
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 18,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 8,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "sortByKey at BinaryClassificationMetrics.scala:189",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 487,
                    "name": "sortByKey at BinaryClassificationMetrics.scala:189",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:10.956GMT",
                    "completionTime": "2023-11-29T07:41:11.402GMT",
                    "stageIds": [
                      999,
                      998
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 486,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:10.184GMT",
                    "completionTime": "2023-11-29T07:41:10.599GMT",
                    "stageIds": [
                      996,
                      997
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 10,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 485,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:09.763GMT",
                    "completionTime": "2023-11-29T07:41:10.162GMT",
                    "stageIds": [
                      994,
                      995
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 10,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 484,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:09.371GMT",
                    "completionTime": "2023-11-29T07:41:09.740GMT",
                    "stageIds": [
                      993,
                      992
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 10,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 483,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:08.945GMT",
                    "completionTime": "2023-11-29T07:41:09.343GMT",
                    "stageIds": [
                      990,
                      991
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 10,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 482,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:08.523GMT",
                    "completionTime": "2023-11-29T07:41:08.918GMT",
                    "stageIds": [
                      988,
                      989
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 10,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 481,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:08.094GMT",
                    "completionTime": "2023-11-29T07:41:08.500GMT",
                    "stageIds": [
                      986,
                      987
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 10,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 480,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 93:\n# Convert the categorical labels in the 'sentiment label' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nsvm_classifier = LinearSVC(maxIter=3, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n\nprint(\"Starting to fit\")\nmodel = svm_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2f}\")\n\neval...",
                    "submissionTime": "2023-11-29T07:41:07.682GMT",
                    "completionTime": "2023-11-29T07:41:08.071GMT",
                    "stageIds": [
                      984,
                      985
                    ],
                    "jobGroup": "93",
                    "status": "SUCCEEDED",
                    "numTasks": 10,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "535c6bb0-84df-49e0-93d2-65b54e1a5ec5"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 93, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Starting to fit\nfinished fitting model\nTest areaUnderROC: 0.51\nTest areaUnderPR: 0.41\nTest Accuracy: 0.68\nTest Weighted Precision: 0.62\nTest Weighted Recall: 0.68\nTest F1: 0.57\n"
        }
      ],
      "execution_count": 91,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701243677024
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, coefficients will contain the weights associated with each feature. Keep in mind that the interpretation of the weights can be more complex for SVM models, and it may not provide a direct measure of feature importance as in other algorithms like decision trees.\n",
        "\n",
        "You can interpret the weights in relation to the magnitude and sign. Larger magnitude weights indicate a stronger influence on the decision boundary, and the sign of the weight can indicate the direction of influence.\n",
        "\n",
        "If feature scaling was applied before training the model, you should consider the scaled values when interpreting the coefficients. Additionally, you may need to consider the absolute values of the coefficients for magnitude-based importance interpretation."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mapping between feature names and their corresponding coefficients\n",
        "feature_coefficients = dict(zip(assembler.getInputCols(),\n",
        "                            model.coefficients))\n",
        "\n",
        "\n",
        "# Print each coefficient next to its corresponding feature\n",
        "for feature_name, coefficient in feature_coefficients.items():\n",
        "    print(f\"Feature: {feature_name}, Coefficient: {coefficient}\")\n",
        "\n",
        "sub_pred = predictions.toPandas()\n",
        "sub_pred.to_csv(dir_name + \"pred_sent_svm.csv\", index = False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 94,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T07:40:37.7162368Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T07:41:16.8168697Z",
              "execution_finish_time": "2023-11-29T07:41:19.3687133Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_5137/1957254409.py:10",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 500,
                    "name": "toPandas at /tmp/ipykernel_5137/1957254409.py:10",
                    "description": "Job group for statement 94:\n# Create a mapping between feature names and their corresponding coefficients\nfeature_coefficients = dict(zip(assembler.getInputCols(),\n                            model.coefficients))\n\n\n# Print each coefficient next to its corresponding feature\nfor feature_name, coefficient in feature_coefficients.items():\n    print(f\"Feature: {feature_name}, Coefficient: {coefficient}\")\n\nsub_pred = predictions.toPandas()\nsub_pred.to_csv(dir_name + \"pred_sent_svm.csv\", index = False)",
                    "submissionTime": "2023-11-29T07:41:16.913GMT",
                    "completionTime": "2023-11-29T07:41:17.383GMT",
                    "stageIds": [
                      1031
                    ],
                    "jobGroup": "94",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "d67be146-dd50-429c-b372-8437cc104a55"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 94, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Feature: num_words, Coefficient: 0.000868626019672739\nFeature: score, Coefficient: 0.0013619016699627539\nFeature: airbnb, Coefficient: 0.007097757497507614\nFeature: rent, Coefficient: 0.5685826071918254\nFeature: gentrification, Coefficient: 0.8830408662448551\nFeature: public_transit, Coefficient: 1.1295979523817465\nFeature: tourists, Coefficient: 0.5925693525651131\n"
        }
      ],
      "execution_count": 92,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701243679429
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest; Predicting Sentiment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the categorical labels in the 'Species' column to numerical values\n",
        "label_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\n",
        "data = label_indexer.fit(spark_sent).transform(spark_sent)\n",
        "\n",
        "# Assemble the feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\n",
        "data = assembler.transform(data)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
        "rf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n",
        "\n",
        "print(\"Starting to fit\")\n",
        "model = rf_classifier.fit(train_data)\n",
        "print(\"finished fitting model\")\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# evaluate\n",
        "evaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "accuracy = evaluator_acc.evaluate(predictions)\n",
        "\n",
        "print(f\"Test areaUnderROC: {accuracy:.2f}\")\n",
        "\n",
        "evaluator_prec = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderPR\")\n",
        "roc_auc = evaluator_prec.evaluate(predictions)\n",
        "print(f\"Test areaUnderPR: {roc_auc:.2f}\")\n",
        "\n",
        "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator_acc.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "evaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "roc_auc = evaluator_prec.evaluate(predictions)\n",
        "print(f\"Test Weighted Precision: {roc_auc:.2f}\")\n",
        "\n",
        "evaluator_rec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "roc_auc = evaluator_rec.evaluate(predictions)\n",
        "print(f\"Test Weighted Recall: {roc_auc:.2f}\")\n",
        "\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions, {evaluator_f1.metricName: \"f1\"})\n",
        "print(f\"Test F1: {f1_score:.2f}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 113,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T08:17:32.4063123Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T08:17:32.5176752Z",
              "execution_finish_time": "2023-11-29T08:17:44.6289739Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 23,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 613,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:42.610GMT",
                    "completionTime": "2023-11-29T08:17:43.101GMT",
                    "stageIds": [
                      1250,
                      1251
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 612,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:41.993GMT",
                    "completionTime": "2023-11-29T08:17:42.517GMT",
                    "stageIds": [
                      1248,
                      1249
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 611,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:41.447GMT",
                    "completionTime": "2023-11-29T08:17:41.907GMT",
                    "stageIds": [
                      1247,
                      1246
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 610,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:40.837GMT",
                    "completionTime": "2023-11-29T08:17:41.375GMT",
                    "stageIds": [
                      1244,
                      1245
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at AreaUnderCurve.scala:44",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 609,
                    "name": "collect at AreaUnderCurve.scala:44",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:40.708GMT",
                    "completionTime": "2023-11-29T08:17:40.722GMT",
                    "stageIds": [
                      1242,
                      1243,
                      1241
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 18,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 16,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at BinaryClassificationMetrics.scala:135",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 608,
                    "name": "first at BinaryClassificationMetrics.scala:135",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:40.688GMT",
                    "completionTime": "2023-11-29T08:17:40.700GMT",
                    "stageIds": [
                      1238,
                      1239,
                      1240
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 17,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 16,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at BinaryClassificationMetrics.scala:237",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 607,
                    "name": "collect at BinaryClassificationMetrics.scala:237",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:40.666GMT",
                    "completionTime": "2023-11-29T08:17:40.679GMT",
                    "stageIds": [
                      1235,
                      1236,
                      1237
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 18,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 16,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at BinaryClassificationMetrics.scala:197",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 606,
                    "name": "count at BinaryClassificationMetrics.scala:197",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:40.626GMT",
                    "completionTime": "2023-11-29T08:17:40.656GMT",
                    "stageIds": [
                      1232,
                      1233,
                      1234
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 18,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 8,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "sortByKey at BinaryClassificationMetrics.scala:189",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 605,
                    "name": "sortByKey at BinaryClassificationMetrics.scala:189",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:40.191GMT",
                    "completionTime": "2023-11-29T08:17:40.618GMT",
                    "stageIds": [
                      1230,
                      1231
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at AreaUnderCurve.scala:44",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 604,
                    "name": "collect at AreaUnderCurve.scala:44",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:40.097GMT",
                    "completionTime": "2023-11-29T08:17:40.111GMT",
                    "stageIds": [
                      1228,
                      1229,
                      1227
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 18,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 16,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at BinaryClassificationMetrics.scala:237",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 603,
                    "name": "collect at BinaryClassificationMetrics.scala:237",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:40.073GMT",
                    "completionTime": "2023-11-29T08:17:40.084GMT",
                    "stageIds": [
                      1224,
                      1225,
                      1226
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 18,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 16,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at BinaryClassificationMetrics.scala:197",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 602,
                    "name": "count at BinaryClassificationMetrics.scala:197",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:40.032GMT",
                    "completionTime": "2023-11-29T08:17:40.063GMT",
                    "stageIds": [
                      1221,
                      1222,
                      1223
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 18,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 8,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "sortByKey at BinaryClassificationMetrics.scala:189",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 601,
                    "name": "sortByKey at BinaryClassificationMetrics.scala:189",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:39.539GMT",
                    "completionTime": "2023-11-29T08:17:40.024GMT",
                    "stageIds": [
                      1220,
                      1219
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:663",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 600,
                    "name": "collectAsMap at RandomForest.scala:663",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:36.333GMT",
                    "completionTime": "2023-11-29T08:17:39.008GMT",
                    "stageIds": [
                      1217,
                      1218
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:663",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 599,
                    "name": "collectAsMap at RandomForest.scala:663",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:35.851GMT",
                    "completionTime": "2023-11-29T08:17:36.303GMT",
                    "stageIds": [
                      1215,
                      1216
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:663",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 598,
                    "name": "collectAsMap at RandomForest.scala:663",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:35.384GMT",
                    "completionTime": "2023-11-29T08:17:35.825GMT",
                    "stageIds": [
                      1214,
                      1213
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:663",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 597,
                    "name": "collectAsMap at RandomForest.scala:663",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:34.944GMT",
                    "completionTime": "2023-11-29T08:17:35.362GMT",
                    "stageIds": [
                      1211,
                      1212
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:663",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 596,
                    "name": "collectAsMap at RandomForest.scala:663",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:34.449GMT",
                    "completionTime": "2023-11-29T08:17:34.921GMT",
                    "stageIds": [
                      1210,
                      1209
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at RandomForest.scala:1054",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 595,
                    "name": "collectAsMap at RandomForest.scala:1054",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:33.964GMT",
                    "completionTime": "2023-11-29T08:17:34.417GMT",
                    "stageIds": [
                      1207,
                      1208
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 15,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 15,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 15,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "aggregate at DecisionTreeMetadata.scala:125",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 594,
                    "name": "aggregate at DecisionTreeMetadata.scala:125",
                    "description": "Job group for statement 113:\n# Convert the categorical labels in the 'Species' column to numerical values\nlabel_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\ndata = label_indexer.fit(spark_sent).transform(spark_sent)\n\n# Assemble the feature columns into a single vector column\nassembler = VectorAssembler(inputCols=[\"num_words\", 'score', 'airbnb', 'rent', 'gentrification', 'public_transit', 'tourists'], outputCol=\"features\")\ndata = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\nrf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, seed=42)\n\nprint(\"Starting to fit\")\nmodel = rf_classifier.fit(train_data)\nprint(\"finished fitting model\")\npredictions = model.transform(test_data)\n\n# evaluate\nevaluator_acc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator_acc.evaluate(predictions)\n\nprint(f\"Test areaUnderROC: {accuracy:.2...",
                    "submissionTime": "2023-11-29T08:17:33.531GMT",
                    "completionTime": "2023-11-29T08:17:33.935GMT",
                    "stageIds": [
                      1206
                    ],
                    "jobGroup": "113",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "91b90ee1-eb5e-4e61-a64e-dc09f8b54497"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 113, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Starting to fit\nfinished fitting model\nTest areaUnderROC: 0.52\nTest areaUnderPR: 0.46\nTest Accuracy: 0.68\nTest Weighted Precision: 0.65\nTest Weighted Recall: 0.68\nTest F1: 0.58\n"
        }
      ],
      "execution_count": 111,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701245864839
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"Users/fall-2023-reddit-project-team-02/data/models/rf-sent\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 114,
              "state": "submitted",
              "livy_statement_state": "running",
              "queued_time": "2023-11-29T08:18:15.9767917Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T08:18:16.070797Z",
              "execution_finish_time": null,
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "RUNNING": 1
                },
                "jobs": [
                  {
                    "displayName": "runJob at SparkHadoopWriter.scala:85",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 614,
                    "name": "runJob at SparkHadoopWriter.scala:85",
                    "description": "Job group for statement 114:\nmodel.save(\"Users/fall-2023-reddit-project-team-02/data/models/rf-sent\")",
                    "submissionTime": "2023-11-29T08:18:16.267GMT",
                    "stageIds": [
                      1252
                    ],
                    "jobGroup": "114",
                    "status": "RUNNING",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 0,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 0,
                    "numActiveStages": 1,
                    "numCompletedStages": 0,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "deaf7367-ceb0-405c-8341-06af311a05fd"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 114, Submitted, Running)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 112,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = model.featureImportances.toArray()\n",
        "\n",
        "# Show feature importance\n",
        "for i, column in enumerate(assembler.getInputCols()):\n",
        "    print(f\"Feature '{column}': {feature_importance[i]:.2f}\")\n",
        "\n",
        "sub_pred = predictions.toPandas()\n",
        "sub_pred.to_csv(dir_name + \"pred_sent_rf.csv\", index = False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 100,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T07:50:15.6354554Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T07:50:15.7240103Z",
              "execution_finish_time": "2023-11-29T07:50:18.1833001Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_5137/2610059376.py:7",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 553,
                    "name": "toPandas at /tmp/ipykernel_5137/2610059376.py:7",
                    "description": "Job group for statement 100:\nfeature_importance = model.featureImportances.toArray()\n\n# Show feature importance\nfor i, column in enumerate(assembler.getInputCols()):\n    print(f\"Feature '{column}': {feature_importance[i]:.2f}\")\n\nsub_pred = predictions.toPandas()\nsub_pred.to_csv(dir_name + \"pred_sent_rf.csv\", index = False)",
                    "submissionTime": "2023-11-29T07:50:15.849GMT",
                    "completionTime": "2023-11-29T07:50:16.354GMT",
                    "stageIds": [
                      1140
                    ],
                    "jobGroup": "100",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "b33cfee3-66c4-40e8-b1c6-4605bbbab385"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 100, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Feature 'num_words': 0.43\nFeature 'score': 0.46\nFeature 'airbnb': 0.00\nFeature 'rent': 0.02\nFeature 'gentrification': 0.00\nFeature 'public_transit': 0.07\nFeature 'tourists': 0.01\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n  Unsupported type in conversion to Arrow: VectorUDT\nAttempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n  warnings.warn(msg)\n"
        }
      ],
      "execution_count": 98,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701244218244
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Sentence Embeddings for Subreddit Predictions"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Transforms raw title into 'document' annotation\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"selftext\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# Step 2: Annotate sentences in the document\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "# Step 3: Tokenize the sentences into tokens\n",
        "tokenizer = Tokenizer()\\\n",
        "    .setInputCols([\"sentence\"])\\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# Step 4: Normalize the tokens\n",
        "normalizer = Normalizer()\\\n",
        "    .setInputCols([\"token\"])\\\n",
        "    .setOutputCol(\"normalized\")\\\n",
        "    .setLowercase(True)\\\n",
        "    .setCleanupPatterns([\"[^\\w\\d\\s]\"])  # Removes any non-word, non-digit, non-whitespace characters\n",
        "\n",
        "# Step 5: Clean tokens using the StopWordsCleaner\n",
        "stop_words_cleaner = StopWordsCleaner().pretrained(\"stopwords_iso\", \"en\")\\\n",
        "    .setInputCols([\"normalized\"])\\\n",
        "    .setOutputCol(\"cleanTokens\")\\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "# Step 6: Load a pretrained Universal Sentence Encoder model\n",
        "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "# Step 7: Load a pretrained SentimentDL model for sentiment analysis\n",
        "# sentimentdl = SentimentDLModel.pretrained(lang=\"en\")\\\n",
        "#     .setInputCols([\"sentence_embeddings\"])\\\n",
        "#     .setOutputCol(\"sentiment\")\n",
        "assembler = VectorAssembler(inputCols=[\"sentence_embeddings\"], outputCol=\"features\")\n",
        "\n",
        "rf_classifier = RandomForestClassifier(labelCol=\"subreddit\", featuresCol=\"features\", numTrees=10)\n",
        "\n",
        "\n",
        "# Construct the ML pipeline\n",
        "pipeline = Pipeline(\n",
        "    stages=[\n",
        "        documentAssembler,\n",
        "        sentenceDetector,\n",
        "        tokenizer,\n",
        "        normalizer,\n",
        "        stop_words_cleaner,\n",
        "        use,\n",
        "        assembler,\n",
        "        rf_classifier\n",
        "    ])\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "5a58fef9-6d5a-48be-a208-b669ac871209",
              "session_id": "49",
              "statement_id": 111,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-29T08:08:47.8664423Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-29T08:08:47.9645413Z",
              "execution_finish_time": "2023-11-29T08:10:52.4997963Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "FAILED": 0,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 590,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 111:\n# Step 1: Transforms raw title into 'document' annotation\ndocumentAssembler = DocumentAssembler()    .setInputCol(\"selftext\")    .setOutputCol(\"document\")\n\n# Step 2: Annotate sentences in the document\nsentenceDetector = SentenceDetector()    .setInputCols([\"document\"])    .setOutputCol(\"sentence\")\n\n# Step 3: Tokenize the sentences into tokens\ntokenizer = Tokenizer()    .setInputCols([\"sentence\"])    .setOutputCol(\"token\")\n\n# Step 4: Normalize the tokens\nnormalizer = Normalizer()    .setInputCols([\"token\"])    .setOutputCol(\"normalized\")    .setLowercase(True)    .setCleanupPatterns([\"[^\\w\\d\\s]\"])  # Removes any non-word, non-digit, non-whitespace characters\n\n# Step 5: Clean tokens using the StopWordsCleaner\nstop_words_cleaner = StopWordsCleaner().pretrained(\"stopwords_iso\", \"en\")    .setInputCols([\"normalized\"])    .setOutputCol(\"cleanTokens\")    .setCaseSensitive(False)\n\n# Step 6: Load a pretrained Universal Sentence Encoder model\nuse = UniversalSentenceEncoder.pretrained(name=\"tfhu...",
                    "submissionTime": "2023-11-29T08:09:55.822GMT",
                    "completionTime": "2023-11-29T08:09:55.914GMT",
                    "stageIds": [
                      1201
                    ],
                    "jobGroup": "111",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 589,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 111:\n# Step 1: Transforms raw title into 'document' annotation\ndocumentAssembler = DocumentAssembler()    .setInputCol(\"selftext\")    .setOutputCol(\"document\")\n\n# Step 2: Annotate sentences in the document\nsentenceDetector = SentenceDetector()    .setInputCols([\"document\"])    .setOutputCol(\"sentence\")\n\n# Step 3: Tokenize the sentences into tokens\ntokenizer = Tokenizer()    .setInputCols([\"sentence\"])    .setOutputCol(\"token\")\n\n# Step 4: Normalize the tokens\nnormalizer = Normalizer()    .setInputCols([\"token\"])    .setOutputCol(\"normalized\")    .setLowercase(True)    .setCleanupPatterns([\"[^\\w\\d\\s]\"])  # Removes any non-word, non-digit, non-whitespace characters\n\n# Step 5: Clean tokens using the StopWordsCleaner\nstop_words_cleaner = StopWordsCleaner().pretrained(\"stopwords_iso\", \"en\")    .setInputCols([\"normalized\"])    .setOutputCol(\"cleanTokens\")    .setCaseSensitive(False)\n\n# Step 6: Load a pretrained Universal Sentence Encoder model\nuse = UniversalSentenceEncoder.pretrained(name=\"tfhu...",
                    "submissionTime": "2023-11-29T08:08:53.751GMT",
                    "completionTime": "2023-11-29T08:08:53.996GMT",
                    "stageIds": [
                      1200
                    ],
                    "jobGroup": "111",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "fe941331-563c-499c-92c8-b407f7a7211e"
            },
            "text/plain": "StatementMeta(5a58fef9-6d5a-48be-a208-b669ac871209, 49, 111, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "stopwords_iso download started this may take some time.\nApproximate size to download 2.1 KB\n[OK!]\ntfhub_use download started this may take some time.\nApproximate size to download 923.7 MB\n[OK!]\n"
        }
      ],
      "execution_count": 109,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701245452652
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Data and Train the Model\n",
        "(train_data, test_data) = df_sub_sample.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train the pipeline\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# Make Predictions and Evaluate\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}